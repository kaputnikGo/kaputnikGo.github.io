## Audio Tracker demo

_(using **Alphonso SDK** (third) example of capturing spoken phonetic alphabet)_

### Introduction
The research published on this page is one part of an ongoing project started in 2015 that has resulted in an audio counter-surveillance Android app that is free, [open source](https://github.com/kaputnikGo/PilferShushJammer) and available to download from either [Google Play](https://play.google.com/store/apps/details?id=cityfreqs.com.pilfershushjammer) or [F-Droid](https://f-droid.org/packages/cityfreqs.com.pilfershushjammer/) store. The research page hosted on the [City Frequencies](https://www.cityfreqs.com.au/pilfer.php) website is the primary location for information regarding this research while more general research is posted to the [@cityfreqs](https://twitter.com/cityfreqs) Twitter account.

The question **"is my device listening to me?"** has been asked many times over the past few years. To determine an answer to this question a demonstration is examined here that uses an Android mobile phone and a game app downloaded for free from Google Play. This demonstration is somewhat technical in nature but is here to allow people to examine both the broader concepts and some of the specificities that may encourage similar research as either a form of confirmation or for additional insights.

The following investigation demonstrates that the Alphonso Software Development Kit (**SDK**) included in the game app records audio using the device microphone and then packages that audio into 64kb files suitable for uploading to servers. Whether this app and its SDK does upload these audio files is primarily determined by the location of the device (Alphonso press statements suggest they only enable this in the USA) and a simple boolean switch in an XML file.

One of the reasons Alphonso might upload the audio is so that they can use the processing power of their servers to run proprietary code that creates new fingerprints to add to their database. An example of this reasoning can be found in Bloomberg's recent article on the [Amazon Alexa device](https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio) that shows that despite the hyperbolic marketing statements heralding the power of "machine learning" and "A.I." there is still a great need for cheap, exploitable human labour to serve as "mechanical Turks".

### Method
After installing the app, approving the permissions and then running it for the first time, the phonetic alphabet was spoken out loud in the vicinity of the mobile device. A ten second portion of this spoken alphabet is captured by the SDK using its default settings and subdivided into three 64kb files of raw audio data. Importing the three raw audio files into Audacity allowed the audio to be played back legibly.

The app, known to contain the Alphonso SDK, has the RECORD_AUDIO permission (as listed on the store page). This SDK is used for Audio Content Recognition (**ACR**) and cross device tracking (**XDT**). ACR is a technique that attempts to derive and match unique aspects (fingerprint) of a given audio sample with a database of known fingerprints.

### ACR
Audio Content Recognition is a system that generates an identifiable and simple "fingerprint" of a complex audio signal. A simplified example is the way the Shazaam app listens to a song that is playing and can then identify the title and artist. To do this, the company offering the service creates a database of audio fingerprints generated by processing audio files through a particular algorithm. This algorithm creates a fingerprint based upon key features of the audio as determined by a spectrogram. 

![Shazaam audio spectrum fingerprint](/images/shazaam-audio-spectrum.jpg)

The left side of the above image shows the initial spectrogram of the audio as a representation of frequencies over time. The right side shows an example of key features being identified, here it is the intensity, or amplitude, of a given frequency as represented by the 'X' marks. Taking a section of these amplitudes and representing them mathematically as a relatively small number provides the fingerprint which can then be stored in a database or easily transmitted over the internet.

Using a similar methodology to the one described above, several ACR companies have created SDKs that are embedded within smart phone apps for the purpose of tracking what television shows and adverts are watched. The recordings usually last a few seconds (5 - 15 secs) and are processed on the device to generate a "finger print" of the audio. This fingerprint is then sent via a network connection to servers for querying. Most of these companies provide analytics to their clients so that they can assess what adverts are being seen and by whom. 

Expanding on this initial purpose, several companies are also using this ACR technique to record and determine what other sounds are present, especially in the background. One example is the La Liga Soccer app that is triggered by GPS location and records audio to determine whether the device is in the vicinity of an unauthorised broadcast of a soccer match - [Engadget La Liga app article](https://www.engadget.com/2018/06/13/spanish-soccer-app-la-liga-spying-pirate-broadcast/).

Another future trend can be found in a Facebook patent application that seeks to record and fingerprint background, ambient sounds to provide a context for devices and their usage - [Fastcompany Facebook ambient audio patent article](https://www.fastcompany.com/90178158/facebook-downplays-ambient-audio-tech-that-can-eavesdrop-on-you). This article also discusses other uses of this technique by companies such as Alphonso and Cambridge Analytica.

### XDT
Cross device tracking is primarily used by advertisers and marketers as a method of measuring where and howeffect their advert dollar spends are. The intention is to prove that, for example, a company's expensive US NFL Superbowl commercial was seen by _n_ number of eyes and that a subset of that number then went on to view the company's website and (ideally) purchase something. Another part of this concept is to promote what is referred to as _nudging_ which suggests that it is possible for a given marketing campaign to "direct" the viewer in a particualr direction over all others. A useful reading on this concept can be found in [Online Manipulation: Hidden Influences in a Digital World](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3306006).

Wikipedia has a simple [definition](https://en.wikipedia.org/wiki/Cross-device_tracking) quoted below:
```markdown
Cross device tracking is a technique in which technology companies 
and advertisers deploy trackers, often in the form of unique identifiers, 
cookies, or even ultrasonic signals, to generate a profile of users 
across multiple devices, not simply one. For example, one such form 
of this tracking uses audio beacons, or inaudible sounds, emitted by 
  one device and recognized through the microphone of the other device.
```

### APP INSTALL
In this particular investigation the Android game app is downloaded to a specific device used for mitmproxy use as well as on a computer via download using Raccoon and static analysis using jadx-gui.

![Victoria Aztec game screen](/images/VictoriaAztec_game.jpg)

Link to Google Play store game app:
[Victoria Aztec Hidden Object](https://play.google.com/store/apps/details?id=com.fgl.adrianmarik.victoriaaztecsfree)

Google Play app store listing includes an Alphonso SDK integration statement:
```markdown
This app is integrated with Alphonso software. Subject to your permission, 
the Alphonso software receives short duration audio samples from the 
microphone on your device. The audio samples never leave your device, 
but are irreversibly encoded (hashed) into digital "fingerprints." The 
fingerprints are compared off-device to commercial content (e.g., TV, 
OTT programming, ads music etc.). If a match is found, then appropriate 
recommendation for content or ads may be delivered to your mobile device. 
The Alphonso software only matches against known audio content and does 
not recognize or understand human conversations or other sounds.
```
Running the app allows access to this "Settings" page:
![VictoriaAztec opt out screen](/images/VictoriaAztec_optout.jpg)

The settings page also provides a link to the privacy policy of the game developer: [Molu Apps Privacy Policy](http://moluapps.com/Privacy_Policy.html).

Permissions requested by the app at install are shown to the user, as well as a brief reasoning:
```java
"This app uses audio to detect TV ads and content and shows appropriate mobile ads"
"android.permission.RECORD_AUDIO"
"android.permission.ACCESS_COARSE_LOCATION"
```

### APK Analysis
A page describing how an Android app can be analysed is found on the [Exodus Privacy website](https://exodus-privacy.eu.org/en/post/exodus_static_analysis/).

Knowing that this app will use the record audio function at some point not related to the gameplay, the next step is to try and determine when that might occur. Recording audio on an Android phone is a function that does not use a large amount of battery power or CPU processing cycles nor storage if only a few files are kept and constantly written over with fresh recordings. However, recording _useful_ audio on an Android device is problematic especially when the intent is to do some sort of processing to that audio to determine its useful characteristics. In this particular case, the specific processing (ACR) is performed to try an identify features of that recorded audio. These specific features are the types of sounds derived from adverts and TV programmes which can be assumed to consist of either music, talking, sound effects or even abstract sounds.

One of the first methods to reduce the amount of possible useless recordings is to restrict the times that the SDK performs the record audio method. As we are dealing with a cross device tracking SDK that triggers adverts, one of the first checks is  whether or not the device is actively being used. To help with this the PilferShush Jammer app, which contains a background services scanner, is used to list any services a particular app has that can run in the background even when the parent app is not running. The service in this case is called **tv.alphonso.service.AlphonsoService** :

![Alphonso service detect](/images/Alphonso-service-detect.jpg)

The AlphonsoService calls sets various parameters including "deviceId", "androidId", "adId", "uuId" and a "PrimeTimeArray". 
```java
    public void initializePrimeTimeArray() {
        this.mPrimeTimeArray = new PrimeTime[5];
        this.mPrimeTimeArray[0] = new PrimeTime();
        this.mPrimeTimeArray[1] = new PrimeTime();
        this.mPrimeTimeArray[2] = new PrimeTime();
        this.mPrimeTimeArray[3] = new PrimeTime();
        this.mPrimeTimeArray[4] = new PrimeTime();
        this.mPrimeTimeArray[0].asFSMBeginEvent = 47;
        this.mPrimeTimeArray[0].asFSMEndEvent = 48;
        this.mPrimeTimeArray[1].asFSMBeginEvent = 49;
        this.mPrimeTimeArray[1].asFSMEndEvent = 50;
        this.mPrimeTimeArray[2].asFSMBeginEvent = 51;
        this.mPrimeTimeArray[2].asFSMEndEvent = 52;
        this.mPrimeTimeArray[3].asFSMBeginEvent = 53;
        this.mPrimeTimeArray[3].asFSMEndEvent = 54;
        this.mPrimeTimeArray[4].asFSMBeginEvent = 55;
        this.mPrimeTimeArray[4].asFSMEndEvent = 56;
    }
```

The primeTimeArray is created and set by **tv.alphonso.service.PrimeTime** which determines when to begin and end any audio captures. 
```java
public PrimeTime() {
    this.begin = "";
    this.end = "";
    this.captureCount = -1;
    this.captureScenarioSleepInterval = -1;
    this.captureScenarioSleepIntervalMax = -1;
    this.captureScenarioSleepIntervalLivetv = -1;
    this.captureScenarioSleepIntervalInhibiterIncrement = -1.0d;
}
```

It also calls **tv.alphonso.service.LocationService** which periodically reports the device location to a server.
```java    
  public void sendLocationUpdate(Location location) {
        Bundle params = new Bundle();
        params.putParcelable("location", location);
        if (this.mAlphonsoClient != null) {
            Message msg = this.mAlphonsoClient.mHandler.obtainMessage();
            msg.what = 3;
            msg.setData(params);
            if (debug) {
                Log.d(TAG, "Sending Location Update to AlphonsoClient.");
            }
            this.mAlphonsoClient.mHandler.sendMessage(msg);
        }
        if (this.mProvClient != null) {
            this.mProvClient.processLocationUpdate();
        }
    }
```

Location services are also used to determine whether the device is stationary, which can be a good indication that the user is sitting down and looking at the screen. The file **tv.alphonso.utils.Utils** has some relevant code:
```java
  locBundle.put("latitude", Double.valueOf(loc.getLatitude()));
  locBundle.put("longitude", Double.valueOf(loc.getLongitude()));
  locBundle.put("altitude", Double.valueOf(loc.getAltitude()))
  locBundle.put("speed", Float.valueOf(loc.getSpeed()));
  locBundle.put("bearing", Float.valueOf(loc.getBearing()));
  locBundle.put("accuracy", Float.valueOf(loc.getAccuracy()));

  tm.getNetworkCountryIso(); 
```

The primetime settings have defaults that are set in **tv.alphonso.utils.PreferencesManager** :
```java  
  public static final String ACS_EVENING_PRIME_TIME_BEGIN_DEFAULT = "19:00";
  public static final String ACS_EVENING_PRIME_TIME_END_DEFAULT = "22:00";
  public static final String ACS_MORNING_PRIME_TIME_BEGIN_DEFAULT = "06:00";
  public static final String ACS_MORNING_PRIME_TIME_END_DEFAULT = "09:00";
```

So from here we can assume that the app has specific times and device attitudes that it will allow it to commence the audio capture process.

When the game app is first started, the SDK inside it gets an updated and obfuscated database file called acr.a.2.1.4.db.zero.mp3 (note the mp3 file extension). The head of the file looks like this: 
```
00000000  41 6c 70 68 6f 6e 73 6f  41 43 52 20 20 20 20 20  |AlphonsoACR     |
00000010  00 00 00 00 06 31 2e 34  2e 30 00 55 01 00 00 1a  |.....1.4.0.U....|
00000020  32 30 31 33 2d 31 30 2d  31 31 20 31 33 3a 33 31  |2013-10-11 13:31|
00000030  3a 31 37 20 2d 30 34 30  30 00 29 35 62 34 66 30  |:17 -0400.)5b4f0|
00000040  31 33 33 31 33 64 32 66  32 64 31 33 30 65 36 39  |13313d2f2d130e69|
00000050  64 63 35 61 30 64 32 65  63 38 61 64 62 31 32 35  |dc5a0d2ec8adb125|
00000060  63 35 37 00 68 00 00 00  ba 00 00 00 00 04 00 00  |c57.h...........|
```

The encrypted database file probably has a schema that conforms to this but it may just be a list of time specific fingerprints:
![database schema](/images/Screenshot_database.png)


Some key parameters were also set in the following file found in the app storage directory:
**alphonso.xml** (redacted. _n.b._: dev-id and advertising-id have same values)
```xml
<map>
    <int name="ad_id_poll_duration" value="1800" />
    <float name="location_accuracy" value="5.3096943" />
    <string name="uuid">XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</string>
    <int name="clock_sync_saved_iterations" value="5" />
    <string name="clock_skew_server_name">clockskew.alphonso.tv</string>
    <float name="location_latitude" value="37.23414" />
    <long name="location_time" value="1541553213743" />
    <int name="acr_mode" value="2" />
    <string name="server_domain">http://tkacr258.alphonso.tv</string>
    <boolean name="audio_file_upload_timedout_flag" value="false" />
    <string name="location_provider">network</string>
    <float name="location_longitude" value="-120.46891" />
    <boolean name="history_flag" value="true" />
    <long name="capture_sleep_time" value="1" />
    <boolean name="audio_file_upload_flag" value="true" />
    <int name="capture_scenario_count" value="0" />
    <string name="dev_id">XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</string>
    <int name="capture_count" value="3" />
    <int name="clock_sync_poll_interval" value="600" />
    <float name="capture_scenario_sleep_inhibiter_increment" value="2.0" />
    <string name="acr_db_filename">acr.a.2.1.4.db.zero.mp3</string>
    <string name="android_id">XXXXXXXXXXXXXXXX</string>
    <string name="advertising_id">XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</string>
    <string name="server_port_ssl"></string>
    <boolean name="capture_power_optimization_mode" value="true" />
    <long name="capture_scenario_sleep_interval" value="8" />
    <long name="capture_duration_ms" value="4000" />
    <int name="acr_shift" value="0" />
    <float name="location_altitude" value="44.54821" />
    <long name="capture_scenario_sleep_interval_livetv_match" value="60" />
    <string name="acr_db_file_dir">/data/user/0/com.fgl.adrianmarik.victoriaaztecsfree/files</string>
    <int name="capture_prebuffer_size" value="0" />
    <int name="db_max_records" value="1000" />
    <string name="acr_db_file_abs_path">/data/user/0/com.fgl.adrianmarik.victoriaaztecsfree/files/acr.a.2.1.4.db.zero.mp3</string>
    <float name="location_speed" value="0.09888778" />
    <boolean name="record_timeouts_flag" value="false" />
    <string name="server_domain_ssl"></string>
    <string name="server_port">4432</string>
    <boolean name="limit_ad_tracking_flag" value="false" />
    <long name="capture_scenario_sleep_interval_max" value="160" />
    <string name="alp_uid">XXXXXXXXX</string>
    <long name="location_poll_interval" value="15" />
</map>
```

In the test conducted, the device was kept stationary, the GPS location was spoofed to a random locale in the USA and the phonetic alphabet (alpha, bravo, charlie, ..., zulu) was spoken aloud.

The recording of audio is initiated by the AlphonsoService which creates **tv.alphonso.audiocaptureservice.AudioCaptureService** that is in charge of the recording functions (acrMode is set to 2: SplitACR).
```java
    public void startRecording() {
        this.mRecorderThread.startRecording(this.mCaptureInstance);
    }
    
    public void enableAcr(int acrMode) {
        if (this.mRecorderThread != null) {
            AudioCaptureClient captureClient;
            switch (acrMode) {
                case 1:
                    captureClient = new LocalACR();
                    break;
                case 2:
                    captureClient = new SplitACR();
                    break;
                case 4:
                    captureClient = new DualACR();
                    break;
                case 8:
                    captureClient = new ServerACR();
                    break;
                default:
                    Log.e(TAG, "Invalid acrType: " + acrMode + ". Cannot instantiate AudioCaptureClient.");
                    return;
            }
            if (acrMode != 8) {
                ((LocalACR) captureClient).setOnBoardAudioDBFilePath(this.mOnBoardAudioDBFilePath);
                ((LocalACR) captureClient).setOnBoardAudioDBFileDir(this.mOnBoardAudioDBFileDir);
                ((LocalACR) captureClient).setAcrShift(this.mAcrShift);
            }
            captureClient.setRecordTimeouts(this.mRecordTimeouts);
            captureClient.init(this.mDeviceId, this.mContext, this.mAudioFPUploadService, this.mAlphonsoClient, this);
            captureClient.setAudioFileUpload(this.mAudioFileUpload);
            captureClient.setAudioFileUploadTimedout(this.mAudioFileUploadTimedout);
            captureClient.mClockSkew = this.mClockSkew;
            this.mRecorderThread.addClient(acrMode, captureClient);
        }
    }
```

The recorder thread sets some parameters that control the way the audio is recorded from the microphone.
**tv.alphonso.audiocaptureservice.RecorderThread** has some relevant code:
```java  
    private static final int RECORDER_AUDIO_BYTES_PER_SEC = 16000;
    private static final int RECORDER_AUDIO_ENCODING = 2;
    private static final int RECORDER_BIG_BUFFER_MULTIPLIER = 16;
    private static final int RECORDER_CHANNELS = 16;
    private static final int RECORDER_SAMPLERATE_44100 = 44100;
    private static final int RECORDER_SAMPLERATE_8000 = 8000;
    private static final int RECORDER_SMALL_BUFFER_MULTIPLIER = 4;
```

The enableACR method calls **tv.alphonso.audiocaptureservice.SplitACR** extends LocalACR which extends AudioCaptureClient. These functions set up the analysis of any audio captured by sending the raw audio data to an included native library called **libacr.so**. Here its via the method acrFingerprintOctet(...).
```java
    public byte send(byte[] bytes, int numBytes, int sampleRate) {
        if (this.mFpStart == 0) {
            this.mFpStart = SystemClock.elapsedRealtime();
        }
        byte[] fingerPrint = acrFingerprintOctet(this.mLocalAudioMatchingToken[this.mCurrentTokenIndex], bytes, numBytes);
        if (!(fingerPrint == null || fingerPrint.length == 0)) {
            if (this.mFpStart != 0 && this.mFpEnd == 0) {
                this.mFpEnd = SystemClock.elapsedRealtime();
                this.mFpDelay = this.mFpEnd - this.mFpStart;
                if (AudioCaptureService.debug) {
                    Log.d(TAG, "Delay = " + Utils.getDurationAsString(this.mFpDelay) + "; Fp-size = " + fingerPrint.length + " for token: " + this.mToken + " timestamp: " + this.mAudioBufferTimestampGMT);
                }
            }
            sendFingerprint(fingerPrint, sampleRate);
            this.mFpStart = 0;
            this.mFpEnd = 0;
        }
        return (byte) 2;
    }

```

The file **tv.alphonso.audiocaptureservice.LocalACR** includes a function of interest:
```java
    public void uploadAudioFileIfRequired(String resultSuffix) {
        if (getOnBoardAudioDBFileDir() == null) {
            return;
        }
        if ((isAudioFileUpload() && getSuccessResultSuffix() != null) || (isAudioFileUploadTimedout() && getSuccessResultSuffix() == null)) {
            String suffix;
            Bundle params = new Bundle();
            params.putString("device_id", this.mDeviceId);
            params.putString("start_time", this.mCaptureInstance.mStartTimeYYMMDD);
            params.putString("acr_type", getAcrType());
            params.putString("token", this.mToken);
            if (getSuccessResultSuffix() != null) {
                suffix = getSuccessResultSuffix();
            } else {
                suffix = resultSuffix;
            }
            params.putString("result_suffix", suffix.replace(' ', '_').replace('&', '_'));
            params.putString("filename", getOnBoardAudioDBFileDir() + "/" + this.mLocalAudioMatchingToken[this.mCurrentTokenIndex] + ".audio.raw");
            Message msg = this.mAlphonsoClient.mHandler.obtainMessage();
            msg.what = 8;
            msg.setData(params);
            if (AudioCaptureService.debug) {
                Log.i(TAG, "Sending AUDIO_CLIP_UPLOAD message to AlphonsoClient Service");
            }
            this.mAlphonsoClient.mHandler.sendMessage(msg);
        }
    }

```

The following line is of particular interest as it refers to the raw audio files that are stored on the device at the location referred to in the **Alphonso.xml** file shown above.
```java
params.putString("filename", getOnBoardAudioDBFileDir() + "/" + this.mLocalAudioMatchingToken[this.mCurrentTokenIndex] + ".audio.raw");
```

These raw audio files are located in the device storage allocated for the app.
![Alphonos ACR folder file list](/images/Alphonso-acr-folder.jpg)


The file **tv.alphonso.alphonsoclient.AlphonsoClient** has several relevant parts of the code that set the names of the audio files that will be stored and potentially uploaded to servers:
```java 
  private void processAudioFileUploadRequest(android.os.Bundle r14) {


  if (args.getBoolean("audio_file_upload")) {
    params.put("filename", 
               getAudioFileUploadFilename(this.mDevId, args.getString("start_time"), 
               args.getString("acr_type"), 
               args.getString("result_suffix")));
  }
  
  public String getAudioFileUploadFilename(String deviceId, String startTime, String acrType, String resultSuffix) {
        StringBuffer filename = new StringBuffer();
        filename.append("android");
        filename.append("-");
        filename.append(deviceId);
        filename.append("-");
        filename.append(this.mAlphonsoUid);
        filename.append("-");
        filename.append(startTime);
        filename.append("-");
        filename.append(acrType);
        filename.append("-");
        filename.append(resultSuffix);
        filename.append(".audio.raw");
        return filename.toString();
    }
```

The file **tv.alphonso.audiocaptureservice.LocalACR** sets the unique part of the filenames:
```java
  protected String[] mLocalAudioMatchingToken = new String[]{"LocalACR1", "LocalACR2", "LocalACR3", "LocalACR4", "LocalACR5"};
```

### Audio
After the above test has been run the captured audio files are copied across to a computer running Audacity. Each of the raw audio files are just that, raw data representing Pulse Code Modulation (**PCM**) audio. PCM is a way of storing the results of an analogue signal to digital data conversion where the value (bit depth) of the amplitude at a given time (1/sample rate) is recorded.

The raw audio has no metadata information to instruct any computer program what the format is so some manual settings are used at the import stage in Audacity.

![Audactiy raw audio settings](/images/import-audio-settings-alphonso.jpg)


The imported audio waveform consisting of three raw files looks like this, where its easy to spot the blocks of measured spoken phonetic alphabet occuring.
![Completed audio join file](/images/Screenshot_3-step-join-speed-redux.png)

The joined audio file is here as an mp3:
[3-step-join-speed-redux.mp3](/audio/3-step-join-speed-redux.mp3)

### Conclusion
The Alphonso SDK included in the free game downloaded from the Google Play store **did** inform the user of its intentions and **did** give the user the option to switch off the audio record function. The SDK **did** record ten seconds of human speech and save it as three raw files with specific, sequential filenames. The SDK has several options, settings and functions that indicate that it **can** upload files that have the same sequential filenames to server(s) attached to sub-domains.
