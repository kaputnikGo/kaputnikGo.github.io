## Audio Tracker demo

_(using **Alphonso SDK** (third) example of capturing spoken phonetic alphabet)_

### Intro
The question of "is my device listening to me?" has been asked many times over the past few years. To help determine an answer to this question a demonstration scenario will be examined here detailing the use of an Android mobile phone and a game app downloaded for free from Google Play.

The app chosen has the RECORD_AUDIO permission (as listed on the store page) and is known to contain the Alphonso SDK. This SDK is used for Audio Content Recognition (**ACR**) and cross device tracking (**XDT**), a technique that attempts to derive and match unique aspects (fingerprint) of a given audio sample with a database of known fingerprints. 

### Method
The following investigation will demonstrate that the Alphonso SDK included in the game app records audio which it then packages into 64kb files suitable for uploading to servers. After installing the app, approving the permissions and then running it for the first time, the phonetic alphabet was spoken out loud in the vicinity of the mobile device. A ten second portion of this spoken alphabet is captured by the SDK using its default settings and subdivided into three 64kb files of raw audio data. Importing the three raw audio files into Audacity as Signed 16 bit PCM, little-endian, mono, 8000Hz allows the audio to be played back legibly.

### XDT
what is it... and why

### ACR
Audio Content Recognition is a system that generates an identifiable and simple "fingerprint" of a complex audio signal. A simplified example is in the way the Shazaam app listens to a song that is playing and can then identify the title and artist. To do this, the company offering the service creates a database of audio fingerprints generated by processing audio files through a particular algorithm. This algorithm creates a fingerprint based upon key features of the audio as determined by a spectrogram. 

![Shazaam audio spectrum fingerprint](/images/shazaam-audio-spectrum.jpg)

The left side of the above image shows the initial spectrogram of the audio as a representation of frequencies over time. The right side shows an example of key features being identified, here it is the intensity of a given frequency. From this a small section can then be indexed to provide a hash which can be stored in a database and queried.

Following a similar methodology, several ACR companies have created SDKs that are embedded within smart phone apps for the purpose of tracking what television shows and adverts are watched. The recordings usually last a few seconds (5 - 15 secs) and are processed on the device to generate a "finger print" of the audio. This fingerprint is then sent via a network connection to servers for querying. Most of these companies provide analytics to their clients so that they can assess what adverts are being seen and by whom. 

Expanding on this initial purpose, several companies are also using this ACR technique to record and determine what other sounds are present, especially in the background. One example is the La Liga Soccer app that is triggered by GPS location and records audio to determine whether the device is in the vicinity of an unauthorised broadcast of a soccer match - [Engadget La Liga app article](https://www.engadget.com/2018/06/13/spanish-soccer-app-la-liga-spying-pirate-broadcast/).

Another future trend can be found in a Facebook patent application that seeks to record and fingerprint background, ambient sounds to provide a context for devices and their usage - [Fastcompany Facebook ambient audio patent article](https://www.fastcompany.com/90178158/facebook-downplays-ambient-audio-tech-that-can-eavesdrop-on-you). This article also discusses other uses of this technique by companies such as Alphonso and Cambridge Analytica.

### APK Analysis
A page describing how an Android app's APK can be analysed is found on the [Exodus Privacy website](https://exodus-privacy.eu.org/en/post/exodus_static_analysis/).

In this particular investigation the Android game app is downloaded to a specific device used for mitmproxy use as well as on a computer via download using Raccoon and static analysis using jadx-gui.

![Victoria Aztec game screen](/images/VictoriaAztec_game.jpg)

Link to Google Play store game app:
[Victoria Aztec Hidden Object](https://play.google.com/store/apps/details?id=com.fgl.adrianmarik.victoriaaztecsfree)

Google Play app store listing Alphonso SDK integration statement:
```markdown
This app is integrated with Alphonso software. Subject to your permission, 
the Alphonso software receives short duration audio samples from the 
microphone on your device. The audio samples never leave your device, 
but are irreversibly encoded (hashed) into digital "fingerprints." The 
fingerprints are compared off-device to commercial content (e.g., TV, 
OTT programming, ads music etc.). If a match is found, then appropriate 
recommendation for content or ads may be delivered to your mobile device. 
The Alphonso software only matches against known audio content and does 
not recognize or understand human conversations or other sounds.
```

![VictoriaAztec opt out screen](/images/VictoriaAztec_optout.jpg)

The Alphonso SDK in the app provides a link to the privacy policy of the game developer on its opt-out screen : [Molu Apps Privacy Policy](http://moluapps.com/Privacy_Policy.html).

Permissions requested by the app at install are shown to the user, as well as a brief reasoning:
```java
"This app uses audio to detect TV ads and content and shows appropriate mobile ads"
"android.permission.RECORD_AUDIO"
"android.permission.ACCESS_COARSE_LOCATION"
```

Knowing that this app will use the record audio function at some point not related to the gameplay, the next step is to try and determine when that might occur. Recording audio on an Android phone is a function that does not use a large amount of battery power or CPU processing cycles nor storage if only a few files are kept and constantly written over with fresh recordings. However, recording _useful_ audio on an Android device is problematic especially when the intent is to do some sort of processing to that audio to determine its useful characteristics. In this case, the specific processing is to try an identify what that audio consists of by using methods such as those discussed above, in the ACR section. The specific characteristics the SDK is interested in are derived from adverts and programmes and can be assumed to consist of either music, talking or even abstract sounds.

One of the first methods to reduce the amount of possible useless recordings, is to restrict when the SDK performs the reocrd audio method. As we are dealing with a cross device tracking SDK that triggers adverts the first check can be a device based query as to whether or not the device is actively being used. To help with this the PilferShush Jammer app, which contains a background services scanner, is used to list any services a particular app has that can run in the background. The service in this case is called **tv.alphonso.service.AlphonsoService** :

![Alphonso service detect](/images/Alphonso-service-detect.jpg)

The AlphonsoService calls sets various parameters including "deviceId", "androidId", "adId", "uuId" and a "PrimeTimeArray". 
```java
    public void initializePrimeTimeArray() {
        this.mPrimeTimeArray = new PrimeTime[5];
        this.mPrimeTimeArray[0] = new PrimeTime();
        this.mPrimeTimeArray[1] = new PrimeTime();
        this.mPrimeTimeArray[2] = new PrimeTime();
        this.mPrimeTimeArray[3] = new PrimeTime();
        this.mPrimeTimeArray[4] = new PrimeTime();
        this.mPrimeTimeArray[0].asFSMBeginEvent = 47;
        this.mPrimeTimeArray[0].asFSMEndEvent = 48;
        this.mPrimeTimeArray[1].asFSMBeginEvent = 49;
        this.mPrimeTimeArray[1].asFSMEndEvent = 50;
        this.mPrimeTimeArray[2].asFSMBeginEvent = 51;
        this.mPrimeTimeArray[2].asFSMEndEvent = 52;
        this.mPrimeTimeArray[3].asFSMBeginEvent = 53;
        this.mPrimeTimeArray[3].asFSMEndEvent = 54;
        this.mPrimeTimeArray[4].asFSMBeginEvent = 55;
        this.mPrimeTimeArray[4].asFSMEndEvent = 56;
    }
```

The primeTimeArray is created and set by tv.alphonso.service.PrimeTime which determines when to begin and end any audio captures. 
```java
public PrimeTime() {
    this.begin = "";
    this.end = "";
    this.captureCount = -1;
    this.captureScenarioSleepInterval = -1;
    this.captureScenarioSleepIntervalMax = -1;
    this.captureScenarioSleepIntervalLivetv = -1;
    this.captureScenarioSleepIntervalInhibiterIncrement = -1.0d;
}
```

It also calls tv.alphonso.service.LocationService which periodically reports the device location to a server.
```java    
  public void sendLocationUpdate(Location location) {
        Bundle params = new Bundle();
        params.putParcelable("location", location);
        if (this.mAlphonsoClient != null) {
            Message msg = this.mAlphonsoClient.mHandler.obtainMessage();
            msg.what = 3;
            msg.setData(params);
            if (debug) {
                Log.d(TAG, "Sending Location Update to AlphonsoClient.");
            }
            this.mAlphonsoClient.mHandler.sendMessage(msg);
        }
        if (this.mProvClient != null) {
            this.mProvClient.processLocationUpdate();
        }
    }
```

Location services are also used to determine whether the device is stationary, which can be a good indication that the user is sitting down and looking at the screen.

tv.alphonso.utils.Utils (edit)
```java
  locBundle.put("latitude", Double.valueOf(loc.getLatitude()));
  locBundle.put("longitude", Double.valueOf(loc.getLongitude()));
  locBundle.put("altitude", Double.valueOf(loc.getAltitude()))
  locBundle.put("speed", Float.valueOf(loc.getSpeed()));
  locBundle.put("bearing", Float.valueOf(loc.getBearing()));
  locBundle.put("accuracy", Float.valueOf(loc.getAccuracy()));
  
  getTMCountryCode(cxt);
  tm.getNetworkCountryIso(); 
```

The primetime settings have defaults that are set in tv.alphonso.utils.PreferencesManager.  :
```java  
  public static final String ACS_EVENING_PRIME_TIME_BEGIN_DEFAULT = "19:00";
  public static final String ACS_EVENING_PRIME_TIME_END_DEFAULT = "22:00";
  public static final String ACS_MORNING_PRIME_TIME_BEGIN_DEFAULT = "06:00";
  public static final String ACS_MORNING_PRIME_TIME_END_DEFAULT = "09:00";
```

So from here we can assume that the app has specific times and device attitudes that it will allow it to commence the audio capture process.

When the game app is first started, the SDK inside it gets an updated and obfuscated database file called acr.a.2.1.4.db.zero.mp3 (note the mp3 file extension). The head of the file looks like this: 
```
00000000  41 6c 70 68 6f 6e 73 6f  41 43 52 20 20 20 20 20  |AlphonsoACR     |
00000010  00 00 00 00 06 31 2e 34  2e 30 00 55 01 00 00 1a  |.....1.4.0.U....|
00000020  32 30 31 33 2d 31 30 2d  31 31 20 31 33 3a 33 31  |2013-10-11 13:31|
00000030  3a 31 37 20 2d 30 34 30  30 00 29 35 62 34 66 30  |:17 -0400.)5b4f0|
00000040  31 33 33 31 33 64 32 66  32 64 31 33 30 65 36 39  |13313d2f2d130e69|
00000050  64 63 35 61 30 64 32 65  63 38 61 64 62 31 32 35  |dc5a0d2ec8adb125|
00000060  63 35 37 00 68 00 00 00  ba 00 00 00 00 04 00 00  |c57.h...........|
```

The encrypted database file probably has a schema that conforms to this but it may just be a list of time specific fingerprints:
![database schema](/images/Screenshot_database.png)


Some key parameters were also set in the following file found in the app storage directory:
alphonso.xml (redacted. note: dev-id and advertising-id have same values)
```xml
<map>
    <int name="ad_id_poll_duration" value="1800" />
    <float name="location_accuracy" value="5.3096943" />
    <string name="uuid">XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</string>
    <int name="clock_sync_saved_iterations" value="5" />
    <string name="clock_skew_server_name">clockskew.alphonso.tv</string>
    <float name="location_latitude" value="37.23414" />
    <long name="location_time" value="1541553213743" />
    <int name="acr_mode" value="2" />
    <string name="server_domain">http://tkacr258.alphonso.tv</string>
    <boolean name="audio_file_upload_timedout_flag" value="false" />
    <string name="location_provider">network</string>
    <float name="location_longitude" value="-120.46891" />
    <boolean name="history_flag" value="true" />
    <long name="capture_sleep_time" value="1" />
    <boolean name="audio_file_upload_flag" value="true" />
    <int name="capture_scenario_count" value="0" />
    <string name="dev_id">XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</string>
    <int name="capture_count" value="3" />
    <int name="clock_sync_poll_interval" value="600" />
    <float name="capture_scenario_sleep_inhibiter_increment" value="2.0" />
    <string name="acr_db_filename">acr.a.2.1.4.db.zero.mp3</string>
    <string name="android_id">XXXXXXXXXXXXXXXX</string>
    <string name="advertising_id">XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX</string>
    <string name="server_port_ssl"></string>
    <boolean name="capture_power_optimization_mode" value="true" />
    <long name="capture_scenario_sleep_interval" value="8" />
    <long name="capture_duration_ms" value="4000" />
    <int name="acr_shift" value="0" />
    <float name="location_altitude" value="44.54821" />
    <long name="capture_scenario_sleep_interval_livetv_match" value="60" />
    <string name="acr_db_file_dir">/data/user/0/com.fgl.adrianmarik.victoriaaztecsfree/files</string>
    <int name="capture_prebuffer_size" value="0" />
    <int name="db_max_records" value="1000" />
    <string name="acr_db_file_abs_path">/data/user/0/com.fgl.adrianmarik.victoriaaztecsfree/files/acr.a.2.1.4.db.zero.mp3</string>
    <float name="location_speed" value="0.09888778" />
    <boolean name="record_timeouts_flag" value="false" />
    <string name="server_domain_ssl"></string>
    <string name="server_port">4432</string>
    <boolean name="limit_ad_tracking_flag" value="false" />
    <long name="capture_scenario_sleep_interval_max" value="160" />
    <string name="alp_uid">XXXXXXXXX</string>
    <long name="location_poll_interval" value="15" />
</map>
```

In the test conducted, the device was kept stationary, the GPS location was spoofed to a random locale in the USA and the phonetic alphabet (alpha, bravo, charlie, ..., zulu) was spoken aloud.

The recording of audio is initiated by the AlphonsoService which creates tv.alphonso.audiocaptureservice.AudioCaptureService that is in charge of the recording functions (acrMode is set to 2: SplitACR).
```java
    public void startRecording() {
        this.mRecorderThread.startRecording(this.mCaptureInstance);
    }
    
    public void enableAcr(int acrMode) {
        if (this.mRecorderThread != null) {
            AudioCaptureClient captureClient;
            switch (acrMode) {
                case 1:
                    captureClient = new LocalACR();
                    break;
                case 2:
                    captureClient = new SplitACR();
                    break;
                case 4:
                    captureClient = new DualACR();
                    break;
                case 8:
                    captureClient = new ServerACR();
                    break;
                default:
                    Log.e(TAG, "Invalid acrType: " + acrMode + ". Cannot instantiate AudioCaptureClient.");
                    return;
            }
            if (acrMode != 8) {
                ((LocalACR) captureClient).setOnBoardAudioDBFilePath(this.mOnBoardAudioDBFilePath);
                ((LocalACR) captureClient).setOnBoardAudioDBFileDir(this.mOnBoardAudioDBFileDir);
                ((LocalACR) captureClient).setAcrShift(this.mAcrShift);
            }
            captureClient.setRecordTimeouts(this.mRecordTimeouts);
            captureClient.init(this.mDeviceId, this.mContext, this.mAudioFPUploadService, this.mAlphonsoClient, this);
            captureClient.setAudioFileUpload(this.mAudioFileUpload);
            captureClient.setAudioFileUploadTimedout(this.mAudioFileUploadTimedout);
            captureClient.mClockSkew = this.mClockSkew;
            this.mRecorderThread.addClient(acrMode, captureClient);
        }
    }
```

The recorder thread sets some parameters that control the way the audio is recorded from the microphone.
tv.alphonso.audiocaptureservice.RecorderThread (edit)
```java  
    private static final int RECORDER_AUDIO_BYTES_PER_SEC = 16000;
    private static final int RECORDER_AUDIO_ENCODING = 2;
    private static final int RECORDER_BIG_BUFFER_MULTIPLIER = 16;
    private static final int RECORDER_CHANNELS = 16;
    private static final int RECORDER_SAMPLERATE_44100 = 44100;
    private static final int RECORDER_SAMPLERATE_8000 = 8000;
    private static final int RECORDER_SMALL_BUFFER_MULTIPLIER = 4;
```

The enableACR method calls tv.alphonso.audiocaptureservice.SplitACR extends LocalACR which extends AudioCaptureClient. These functions set up the analysis of any audio captured by sending the raw audio data to an included native library called **libacr.so**. here its via the method acrFingerprintOctet(...)
```java
    public byte send(byte[] bytes, int numBytes, int sampleRate) {
        if (this.mFpStart == 0) {
            this.mFpStart = SystemClock.elapsedRealtime();
        }
        byte[] fingerPrint = acrFingerprintOctet(this.mLocalAudioMatchingToken[this.mCurrentTokenIndex], bytes, numBytes);
        if (!(fingerPrint == null || fingerPrint.length == 0)) {
            if (this.mFpStart != 0 && this.mFpEnd == 0) {
                this.mFpEnd = SystemClock.elapsedRealtime();
                this.mFpDelay = this.mFpEnd - this.mFpStart;
                if (AudioCaptureService.debug) {
                    Log.d(TAG, "Delay = " + Utils.getDurationAsString(this.mFpDelay) + "; Fp-size = " + fingerPrint.length + " for token: " + this.mToken + " timestamp: " + this.mAudioBufferTimestampGMT);
                }
            }
            sendFingerprint(fingerPrint, sampleRate);
            this.mFpStart = 0;
            this.mFpEnd = 0;
        }
        return (byte) 2;
    }

```

the file tv.alphonso.audiocaptureservice.LocalACR includes a function of interest:
```java
    public void uploadAudioFileIfRequired(String resultSuffix) {
        if (getOnBoardAudioDBFileDir() == null) {
            return;
        }
        if ((isAudioFileUpload() && getSuccessResultSuffix() != null) || (isAudioFileUploadTimedout() && getSuccessResultSuffix() == null)) {
            String suffix;
            Bundle params = new Bundle();
            params.putString("device_id", this.mDeviceId);
            params.putString("start_time", this.mCaptureInstance.mStartTimeYYMMDD);
            params.putString("acr_type", getAcrType());
            params.putString("token", this.mToken);
            if (getSuccessResultSuffix() != null) {
                suffix = getSuccessResultSuffix();
            } else {
                suffix = resultSuffix;
            }
            params.putString("result_suffix", suffix.replace(' ', '_').replace('&', '_'));
            params.putString("filename", getOnBoardAudioDBFileDir() + "/" + this.mLocalAudioMatchingToken[this.mCurrentTokenIndex] + ".audio.raw");
            Message msg = this.mAlphonsoClient.mHandler.obtainMessage();
            msg.what = 8;
            msg.setData(params);
            if (AudioCaptureService.debug) {
                Log.i(TAG, "Sending AUDIO_CLIP_UPLOAD message to AlphonsoClient Service");
            }
            this.mAlphonsoClient.mHandler.sendMessage(msg);
        }
    }

```

The following line is of particular interest as it refers to the raw audio files that are stored on the device at the location referred to in the Alphonso.xml file shown above.
```java
params.putString("filename", getOnBoardAudioDBFileDir() + "/" + this.mLocalAudioMatchingToken[this.mCurrentTokenIndex] + ".audio.raw");
```
These raw audio files are located in the device storage allocated for the app.
![Alphonos ACR folder file list](/images/Alphonso-acr-folder.jpg)


tv.alphonso.alphonsoclient.AlphonsoClient (edit)
```java 
  if (args.getBoolean("audio_file_upload")) {
    params.put("filename", 
               getAudioFileUploadFilename(this.mDevId, args.getString("start_time"), 
               args.getString("acr_type"), 
               args.getString("result_suffix")));
  }
  
  public String getAudioFileUploadFilename(String deviceId, String startTime, String acrType, String resultSuffix) {
        StringBuffer filename = new StringBuffer();
        filename.append("android");
        filename.append("-");
        filename.append(deviceId);
        filename.append("-");
        filename.append(this.mAlphonsoUid);
        filename.append("-");
        filename.append(startTime);
        filename.append("-");
        filename.append(acrType);
        filename.append("-");
        filename.append(resultSuffix);
        filename.append(".audio.raw");
        return filename.toString();
    }
```

tv.alphonso.audiocaptureservice.LocalACR (edit)
```java
  protected String[] mLocalAudioMatchingToken = new String[]{"LocalACR1", "LocalACR2", "LocalACR3", "LocalACR4", "LocalACR5"};
```

### Audio
After the above test has been run the captured audio files are copied across to a computer running Audacity. Each of the raw audio files are just that, raw data representing Pulse Code Modulation (**PCM**) audio. PCM is a way of storing the results of an analogue signal to digital data conversion where the value (bit depth) of the amplitude at a given time (1/sample rate) is recorded.

The raw audio has no metadata information to instruct any computer program what the format is so some manual settings are used at the import stage in Audacity.

![Audactiy raw audio settings](/images/import-audio-settings-alphonso.jpg)


The imported audio waveform consisting of three raw files looks like this, where its easy to spot the blocks of measured spoken phonetic alphabet occuring.
![Completed audio join file](/images/Screenshot_3-step-join-speed-redux.png)

The joined audio file is here as an mp3:
[3-step-join-speed-redux.mp3](/audio/3-step-join-speed-redux.mp3)

All thats left to do is figure out if this audio was uploaded to their servers...
